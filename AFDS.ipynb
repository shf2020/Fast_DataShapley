{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and service model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch \n",
    "import torchvision \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import numpy as np \n",
    "import os \n",
    "import os.path \n",
    "from torch.utils.data import DataLoader \n",
    "from tqdm.auto import tqdm \n",
    "from copy import deepcopy \n",
    "from resnet import ResNet18 \n",
    "# Select device\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device('cuda')\n",
    "#加载mnist\n",
    "def get_trans():\n",
    "    # 设置一个转换的集合，先把数据转换到tensor，再归一化为均值.5，标准差.5的正态分布\n",
    "    trans = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),  # ToTensor方法把[0,255]变成[0,1]\n",
    "            torchvision.transforms.Normalize( [0.5], [0.5] )\n",
    "            # 变成mean(均值)=0，std（标准差standard deviation）=1的分布\n",
    "        ]\n",
    "    )\n",
    "    return trans\n",
    " \n",
    "DOWNLOAD_MNIST=True\n",
    "train_set= torchvision.datasets.MNIST( root=\"./mnist\",  # 设置数据集的根目录\n",
    "    train=True,  # 是否是训练集\n",
    "    transform=get_trans(),  # 对数据进行转换\n",
    "    download=DOWNLOAD_MNIST\n",
    "                                         )\n",
    "val_set = torchvision.datasets.MNIST( root=\"./mnist\", train=False,  # 测试集，所以false\n",
    "    transform=get_trans(), download=DOWNLOAD_MNIST\n",
    "                                       )\n",
    "print(len(train_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定训练集大小\n",
    "train_size = 100\n",
    "train_data = []\n",
    "train_data_label = []\n",
    "#每个类别取相同数量\n",
    "for c in range(10):\n",
    "    num = 0\n",
    "    for i in range(50000):\n",
    "        if train_set[i][1] == c:\n",
    "            num+=1\n",
    "            train_data.append(train_set[i][0].numpy())\n",
    "            train_data_label.append(train_set[i][1])\n",
    "            if num == train_size//10:\n",
    "                break\n",
    "\n",
    "train_data = torch.tensor(train_data)\n",
    "train_data_label = torch.tensor(train_data_label)\n",
    "\n",
    "train_dataset = TensorDataset(train_data,train_data_label)\n",
    "print(\"训练集大小：\",len(train_dataset))\n",
    "\n",
    "#指定验证集大小 （同时作为解释器的验证集）\n",
    "val_size = 100\n",
    "val_data = []\n",
    "val_data_label = []\n",
    "#每个类别取相同数量\n",
    "for c in range(10):\n",
    "    num = 0\n",
    "    for i in range(10000):\n",
    "        if train_set[i+50000][1] == c:\n",
    "            num+=1\n",
    "            val_data.append(train_set[i+50000][0].numpy())\n",
    "            val_data_label.append(train_set[i+50000][1])\n",
    "            if num == val_size//10:\n",
    "                break\n",
    "\n",
    "val_data = torch.tensor(val_data)\n",
    "val_data_label = torch.tensor(val_data_label)\n",
    "\n",
    "val_dataset = TensorDataset(val_data,val_data_label)\n",
    "print(\"验证集大小：\",len(val_dataset))\n",
    "\n",
    "#指定测试集大小（同时作为解释器的训练集）\n",
    "test_size = 1000\n",
    "test_data = []\n",
    "test_data_label = []\n",
    "#每个类别取相同数量\n",
    "for c in range(10):\n",
    "    num = 0\n",
    "    for i in range(10000):\n",
    "        if val_set[i][1] == c:\n",
    "            num+=1\n",
    "            test_data.append(val_set[i][0].numpy())\n",
    "            test_data_label.append(val_set[i][1])\n",
    "            if num == test_size//10:\n",
    "                break\n",
    "\n",
    "test_data = torch.tensor(test_data)\n",
    "test_data_label = torch.tensor(test_data_label)\n",
    "\n",
    "test_dataset = TensorDataset(test_data,test_data_label)\n",
    "print(\"测试集大小：\",len(test_dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"分类模型测试集大小\",train_data.size(), train_data_label.size())\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1=nn.Sequential(\n",
    "            nn.Conv2d(          #(1,28,28)\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2   #padding=(kernelsize-stride)/2\n",
    "            ),#(16,28,28)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)#(16,14,14)\n",
    " \n",
    "        )\n",
    "        self.conv2=nn.Sequential(#(16,14,14)\n",
    "            nn.Conv2d(16,32,5,1,2),#(32,14,14)\n",
    "            nn.ReLU(),#(32,14,14)\n",
    "            nn.MaxPool2d(2)#(32,7,7)\n",
    "        )\n",
    "        self.out=nn.Linear(32*7*7,10)\n",
    "    def forward(self,x):\n",
    "        x = self.conv1( x )\n",
    "        x = self.conv2( x ) #(batch,32,7,7)\n",
    "        x=x.view(x.size(0),-1) #(batch,32*7*7)\n",
    "        output=self.out(x)\n",
    "        return output\n",
    "print(\"start\")\n",
    "EPOCH=50#总的训练次数\n",
    "BATCH_SIZE=20#批次的大小\n",
    "LR=0.0001#学习率#交叉熵损失函数不需要太大的学习率\n",
    "\n",
    "train_loader=DataLoader(\n",
    "                train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                )\n",
    "val_loader=DataLoader(\n",
    "            val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "            ) \n",
    "test_loader=DataLoader(\n",
    "            test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "            ) \n",
    "\n",
    "grand_model_path = 'model/grand_mnist_{}_shuffle.pt'.format(train_size)\n",
    "null_model_path = 'model/null_mnist_{}_shuffle.pt'.format(train_size)\n",
    "\n",
    "if os.path.isfile(grand_model_path):\n",
    "    print('Loading saved class model')\n",
    "    #加载大模型和空模型\n",
    "    grand_model = torch.load(grand_model_path).cuda()\n",
    "    null_model = torch.load(null_model_path).cuda()\n",
    "\n",
    "else:\n",
    "    #训练过程\n",
    "    cnn=CNN().cuda()\n",
    "    null_model = cnn\n",
    "    torch.save(null_model, null_model_path)\n",
    "    optimizer=torch.optim.Adam(cnn.parameters(),lr=LR)\n",
    "    loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "    for ep in range(EPOCH):\n",
    "        # 记录把所有数据集训练+测试一遍需要多长时间\n",
    "        startTick =  time.perf_counter()\n",
    "        for img, label in train_loader:  # 对于训练集的每一个batch\n",
    "            # print(img,label)  \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "            out = cnn( img )  # 送进网络进行输出\n",
    "            loss = loss_function( out, label )  # 获得损失\n",
    "            # print(loss)\n",
    "            optimizer.zero_grad()  # 梯度归零\n",
    "            loss.backward()  # 反向传播获得梯度，但是参数还没有更新\n",
    "            optimizer.step()  # 更新梯度\n",
    "        num_correct = 0  # 正确分类的个数，在测试集中测试准确率\n",
    "        for img, label in val_loader:       \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "            out = cnn( img )  # 获得输出\n",
    "\n",
    "            _, prediction = torch.max( out, 1 )\n",
    "            # torch.max()返回两个结果，\n",
    "            # 第一个是最大值，第二个是对应的索引值；\n",
    "            # 第二个参数 0 代表按列取最大值并返回对应的行索引值，1 代表按行取最大值并返回对应的列索引值。\n",
    "            num_correct += (prediction == label).sum()  # 找出预测和真实值相同的数量，也就是以预测正确的数量\n",
    "\n",
    "        accuracy = num_correct.cpu().numpy() / val_size  # 计算正确率，num_correct是gpu上的变量，先转换成cpu变量\n",
    "        timeSpan =  time.perf_counter() - startTick\n",
    "        print( \"第%d迭代期，验证集准确率为%f,耗时%dS\" % (ep + 1, accuracy, timeSpan) )\n",
    "            \n",
    "    grand_model = cnn\n",
    "    torch.save(grand_model, grand_model_path)\n",
    "\n",
    "  \n",
    "num_correct = 0    \n",
    "num_correct1 = 0  # 正确分类的个数，在测试集中测试准确率\n",
    "num_correct2 = 0 \n",
    "for img, label in train_loader:       \n",
    "    img = img.cuda()\n",
    "    label = label.cuda()\n",
    "    out1 = grand_model(img)  # 获得输出\n",
    "    _, prediction1 = torch.max( out1, 1 )\n",
    "    num_correct += (prediction1 == label).sum() \n",
    "    accuracy = num_correct.cpu().numpy() / train_size\n",
    "    \n",
    "for img, label in test_loader:       \n",
    "    img = img.cuda()\n",
    "    label = label.cuda()\n",
    "    out1 = grand_model(img)  # 获得输出\n",
    "    out2 = null_model(img)\n",
    "    _, prediction1 = torch.max( out1, 1 )\n",
    "    _, prediction2 = torch.max( out2, 1 )\n",
    "    # torch.max()返回两个结果，\n",
    "    # 第一个是最大值，第二个是对应的索引值；\n",
    "    # 第二个参数 0 代表按列取最大值并返回对应的行索引值，1 代表按行取最大值并返回对应的列索引值。\n",
    "    num_correct1 += (prediction1 == label).sum()  # 找出预测和真实值相同的数量，也就是以预测正确的数量\n",
    "    num_correct2 += (prediction2 == label).sum()\n",
    "accuracy1 = num_correct1.cpu().numpy() / test_size  # 计算正确率，num_correct是gpu上的变量，先转换成cpu变量\n",
    "accuracy2 = num_correct2.cpu().numpy() / test_size \n",
    "print( \"grand_model在训练集上准确率为%f,在测试集上准确率为%f,null_model准确率为%f\" % (accuracy,accuracy1, accuracy2) )\n",
    "       \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train explainer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "from AFDS import DataFastSHAP\n",
    "print(\"cuda:\",torch.cuda.is_available(),torch.cuda.device_count(),\"个\")\n",
    "# Check for model\n",
    "num_features = train_size\n",
    "#加载大模型和空模型\n",
    "grand_model = torch.load(grand_model_path).cuda()\n",
    "null_model = torch.load(null_model_path).cuda()\n",
    "#选取解释器训练集\n",
    "test_size = 1000\n",
    "test_data = []\n",
    "test_data_label = []\n",
    "for c in range(10):\n",
    "    num = 0\n",
    "    for i in range(10000):\n",
    "        if val_set[i][1] == c and torch.max( \n",
    "                grand_model(val_set[i][0].resize(1,1,28,28).cuda()), 1 )[1] == c:\n",
    "            num+=1\n",
    "            test_data.append(val_set[i][0].numpy())\n",
    "            test_data_label.append(val_set[i][1])\n",
    "            if num == test_size//10:\n",
    "                break\n",
    "\n",
    "test_data = torch.tensor(test_data)\n",
    "test_data_label = torch.tensor(test_data_label)\n",
    "\n",
    "test_dataset = TensorDataset(test_data,test_data_label)\n",
    "print(\"解释器训练集大小：\",len(test_dataset))\n",
    "explainer_path = 'model/AFDS_mnist_100_explainer_10.pt'\n",
    "loss_path = 'exp_fast_datashapley/AFDS_mnist_{}_explainer_10'.format(train_size)\n",
    "\n",
    "# explainer_path = 'model/AFDS_mnist_100_explainer_value_avg_10.pt'\n",
    "# loss_path = 'exp_fast_datashapley/AFDS_mnist_{}_explainer'.format(train_size)\n",
    "\n",
    "if os.path.isfile(explainer_path):\n",
    "    print('Loading saved explainer model')\n",
    "    explainer = torch.load(explainer_path).cuda()\n",
    "    fastshap = DataFastSHAP(explainer, explainer_path, loss_path, grand_model, null_model, train_data, train_data_label, num_features, normalization='additive',\n",
    "                        link=nn.Softmax(dim=-1))\n",
    "\n",
    "else:\n",
    "    # Create explainer model\n",
    "    explainer = UNet(n_classes=num_features, num_down=2, num_up=1, num_convs=3).to(device)\n",
    "     \n",
    "    # Set up FastSHAP object\n",
    "    fastshap = DataFastSHAP(explainer, explainer_path, loss_path, grand_model, null_model, train_data, train_data_label, num_features, normalization='additive',\n",
    "                        link=nn.Softmax(dim=-1))\n",
    "\n",
    "    # Train\n",
    "    startTick =  time.perf_counter()\n",
    "    fastshap.train(\n",
    "        test_dataset,\n",
    "        val_data,\n",
    "        val_data_label,\n",
    "        class_lr = 10*LR,\n",
    "        batch_size=10,\n",
    "        num_samples=1,\n",
    "        max_epochs=300,\n",
    "        stop_epoch = 10,\n",
    "        validation_samples=1,\n",
    "        verbose=True)\n",
    "    timeSpan =  time.perf_counter() - startTick\n",
    "    print( \"训练解释器耗时: %dS\" % (timeSpan) )\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one image from each class\n",
    "test_x = []\n",
    "test_x_label = []\n",
    "shap_test_size = 10\n",
    "for c in range(10):\n",
    "    num = 0\n",
    "    for i in range(len(val_set)):\n",
    "        if val_set[i][1] == c and torch.max( grand_model(val_set[i][0].resize(1,1,28,28).cuda()), 1 )[1] == c:\n",
    "            num+=1\n",
    "            test_x.append(val_set[i][0].numpy())\n",
    "            test_x_label.append(val_set[i][1])\n",
    "            if num == shap_test_size//10:\n",
    "                break\n",
    "test_x = torch.tensor(test_x)\n",
    "test_x_label = torch.tensor(test_x_label)\n",
    "\n",
    "print(test_x.size())\n",
    "print(test_x_label)\n",
    "# Get explanations\n",
    "# values = fastshap.shap_values(test_x.cuda(), test_x_label.cuda(), grand_model, null_model)\n",
    "values = fastshap.shap_values(test_x.cuda(), grand_model, null_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values.shape)\n",
    "label_values = []\n",
    "for i in range(10):    \n",
    "    label_values.append(values[i,:,test_x_label[i]]) \n",
    "print(np.array(label_values).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "num_classes = 10\n",
    "x = test_x\n",
    "y = test_x_label\n",
    "top_k = 10 #取前十个贡献大的训练数据\n",
    "idxs = []  #存放索引值\n",
    "train_data_topk = []\n",
    "train_data_label_topk = []\n",
    "label_values_topk = []\n",
    "for i in range(10):\n",
    "    idxs.append(np.array(label_values[i]).argsort()[::-1][0:top_k])\n",
    "# print(idxs)\n",
    "for i in range(len(idxs)): \n",
    "    # print(train_data.numpy().shape,train_data_label.numpy().shape) \n",
    "    train_data_topk.append(train_data.numpy()[list(idxs[i])])\n",
    "    train_data_label_topk.append(train_data_label.numpy()[list(idxs[i])])\n",
    "    label_values_topk.append(label_values[i][list(idxs[i])])\n",
    "\n",
    "train_data_topk = np.array(train_data_topk)\n",
    "train_data_label_topk = np.array(train_data_label_topk)\n",
    "label_values_topk = np.array( label_values_topk)\n",
    "# print(label_values_topk)\n",
    "# print(train_data_topk.shape,train_data_label_topk.shape, label_values_topk.shape)\n",
    "#分别对不同类别的一个样本画出前十最相关的训练图像\n",
    "num_classes = 3\n",
    "fig, axarr = plt.subplots(num_classes, 11, figsize=(22, 2*num_classes))\n",
    "for row in range(num_classes):\n",
    "    \n",
    "    classes_label = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    classes = [ 'top1', 'top2', 'top3', 'top4', 'top5', 'top6', 'top7', 'top8', 'top9', 'top10']\n",
    "    mean = np.array([0.5] )[:, np.newaxis, np.newaxis]\n",
    "    std = np.array([0.5])[:, np.newaxis, np.newaxis]\n",
    "    im = test_x[row].numpy() * std + mean\n",
    "    im = im.transpose(1, 2, 0).astype(float)\n",
    "    im = np.clip(im, a_min=0, a_max=1)\n",
    "    axarr[row, 0].imshow(im, vmin=0, vmax=1)\n",
    "    axarr[row, 0].set_xticks([])\n",
    "    axarr[row, 0].set_yticks([])\n",
    "    # class labels\n",
    "    axarr[row, 0].set_ylabel('{}'.format(y[row]), fontsize=20)\n",
    "    \n",
    "    # Explanations \n",
    "    for col in range(top_k):\n",
    "        im = train_data_topk[row, col,:,:,:] * std + mean\n",
    "        im = im.transpose(1, 2, 0).astype(float)\n",
    "        axarr[row, col + 1].imshow(im, vmin=0, vmax=1)\n",
    "        axarr[row, col + 1].set_xticks([])\n",
    "        axarr[row, col + 1].set_yticks([])\n",
    "        # topk labels\n",
    "        if row == 0:\n",
    "            axarr[row, col + 1].set_title('{}'.format(classes[y[col]]), fontsize=20)\n",
    "            if col == 0:\n",
    "                axarr[row, col].set_title('test', fontsize=20)\n",
    "        # shapley values\n",
    "        # axarr[row, col + 1].set_xlabel('label:{},shap:{:.2f}'.format(train_data_label_topk[row, col],label_values_topk[row, col]), fontsize=12, fontweight='bold')\n",
    "        axarr[row, col + 1].set_xlabel('o:{},p:{},s:{:.3f}'.format(train_data_label_topk[row, col],torch.max(grand_model(torch.tensor(train_data_topk[row, col]).resize(1,1,28,28).cuda()), 1 )[1].item(),label_values_topk[row, col]), fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"exp_fast_datashapley/Visualize_AFDS_mnist_{}.png\".format(train_size))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_plots(X, y, vals, link,test_x, y_ture, name=None, \n",
    "                        num_plot_markers=20, sources=None):\n",
    "    \"\"\"Plots the effect of removing valuable points.\n",
    "     X:training data\n",
    "     y:training data label\n",
    "     y_ture: 当前测试样本真实标签\n",
    "    Args:\n",
    "        vals: A list of different valuations of data points each\n",
    "                in the format of an array in the same length of the data.\n",
    "        name: Name of the saved plot if not None.\n",
    "        num_plot_markers: number of points in each plot.\n",
    "        sources: If values are for sources of data points rather than\n",
    "                individual points. In the format of an assignment array\n",
    "                or dict.\n",
    "                \n",
    "    Returns:\n",
    "        Plots showing the change in performance as points are removed\n",
    "        from most valuable to least.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = 8,8\n",
    "    plt.rcParams['font.size'] = 25\n",
    "    plt.xlabel('Fraction of training data removed (%)')\n",
    "    plt.ylabel('Value loss', fontsize=20)\n",
    "    if not isinstance(vals, list) and not isinstance(vals, tuple):\n",
    "        vals = [vals]\n",
    "    if sources is None:\n",
    "        sources = {i:np.array([i]) for i in range(len(X))}\n",
    "    elif not isinstance(sources, dict):\n",
    "        sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "    vals_sources = [np.array([np.sum(np.array(val)[sources[i]]) \n",
    "                                for i in range(len(sources.keys()))])\n",
    "                for val in vals]\n",
    "    if len(sources.keys()) < num_plot_markers:\n",
    "        num_plot_markers = len(sources.keys()) - 1\n",
    "    plot_points = np.arange(\n",
    "        0, \n",
    "        max(len(sources.keys()) - 10, num_plot_markers),\n",
    "        max(len(sources.keys())//num_plot_markers, 1)\n",
    "    )\n",
    "    # print(plot_points)\n",
    "    # print(len(vals))\n",
    "    perfs = [portion_performance( X,y,\n",
    "        # np.argsort 从小到大的索引值 [::-1]从大到小\n",
    "        np.argsort(vals_source)[::-1], plot_points,link, test_x, y_ture, sources=sources)\n",
    "                for vals_source in vals_sources]\n",
    "    # print(torch.tensor(perfs).shape)\n",
    "    rnd = np.mean([portion_performance(X,y,\n",
    "        np.random.permutation(np.argsort(vals_sources[0])[::-1]),\n",
    "        plot_points,link, test_x, y_ture, sources=sources) for _ in range(1)], 0)\n",
    "\n",
    "    plt.plot(plot_points/len(X) * 100, perfs[0] , \n",
    "                '-', lw=5, ms=10, color='b')\n",
    "    \n",
    "    plt.plot(plot_points/len(X) * 100, rnd , \n",
    "                ':', lw=5, ms=10, color='r') \n",
    "    # print(\"4----------\",len(vals))\n",
    "    if len(vals)==3:\n",
    "        plt.plot(plot_points/len(X) * 100, perfs[1] , \n",
    "                '-.', lw=5, ms=10, color='g') \n",
    "        plt.plot(plot_points/len(X) * 100, perfs[-1], \n",
    "                    '--', lw=5, ms=10, color='orange')\n",
    "        legends = ['GFDS+', 'Random','DataShapley ', 'LOO']\n",
    "    elif len(vals)==2:\n",
    "        legends = ['GFDS+', 'Random','DataShapley ']\n",
    "        plt.plot(plot_points/len(X) * 100, perfs[1] , \n",
    "                '-.', lw=5, ms=10, color='g') \n",
    "    else:\n",
    "        legends = ['GFDS+', 'Random']   \n",
    "    plt.legend(legends)\n",
    "    # print('ours:',perfs[0])\n",
    "    # if train_size==100:\n",
    "    #     print('DataShapley:',perfs[1])\n",
    "    #     print('loo:',perfs[-1])\n",
    "    # print('random:',rnd)\n",
    "    perfs.append(rnd)\n",
    "    \n",
    "    # plt.savefig( '{}.png'.format(name),\n",
    "    #             bbox_inches = 'tight')\n",
    "    # plt.close()\n",
    "    # plt.show()\n",
    "    return perfs\n",
    "       \n",
    "def portion_performance(X,y,idxs, plot_points, link,test_x, y_ture, sources=None):\n",
    "    \"\"\"Given a set of indexes, starts removing points from \n",
    "    the first elemnt and evaluates the new model after\n",
    "    removing each point.\"\"\"\n",
    "    if sources is None:\n",
    "        sources = {i:np.array([i]) for i in range(len(X))}\n",
    "    elif not isinstance(sources, dict):\n",
    "        sources = {i:np.where(sources==i)[0] for i in set(sources)}\n",
    "    scores = []\n",
    "    # print(\"1------------------\")\n",
    "    # print(plot_points)\n",
    "    for i in range(len(plot_points)):\n",
    "        keep_idxs = np.concatenate([sources[idx] for idx \n",
    "                                    in idxs[plot_points[i]:]], -1)\n",
    "        # print(keep_idxs,len(keep_idxs))\n",
    "        origin_idxs = keep_idxs[np.argsort(keep_idxs)]\n",
    "        # print(\"-------------\",origin_idxs)\n",
    "        X_batch, y_batch = X[origin_idxs], y[origin_idxs]\n",
    "        X_S = torch.tensor(X_batch).cuda()\n",
    "        y_X = torch.tensor(y_batch).cuda()\n",
    "        class_train_dataset = TensorDataset(X_S,y_X)\n",
    "        # print(X_S.size(),y_X.size())\n",
    "        class_train_loader=DataLoader(\n",
    "            class_train_dataset, batch_size=20, shuffle=True, \n",
    "            )\n",
    "        null_model = torch.load(null_model_path).cuda()\n",
    "        net = null_model.cuda()\n",
    "        class_optimizer=torch.optim.Adam(net.parameters(),lr=0.0001)\n",
    "        loss_function=nn.CrossEntropyLoss()\n",
    "        # print(\"2------------------\")\n",
    "        for ep in range(50):\n",
    "                # 记录把所有数据集训练+测试一遍需要多长时间 \n",
    "            for img, label in class_train_loader:  # 对于训练集的每一个batch\n",
    "                # print(img,label)  \n",
    "                img = img.cuda()\n",
    "                label = label.cuda()\n",
    "                out = net( img )  # 送进网络进行输出\n",
    "                # print(img.size(),label.size())\n",
    "                loss = loss_function( out, label ) \n",
    "                class_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                class_optimizer.step()\n",
    "        # print(test_x.view(1,1,28,28).shape)\n",
    "        # print(net(test_x.view(1,1,28,28).cuda()))\n",
    "        # print(\"3------------------\",link(net(test_x.view(1,1,28,28).cuda()))[0])\n",
    "        scores.append(nn.CrossEntropyLoss()(net(test_x.view(1,1,28,28).cuda()),y_ture.view(1).cuda()).item())\n",
    "    # print(scores)\n",
    "    return np.array(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试第i个生成样本\n",
    "H_eta = []   \n",
    "for i in range(shap_test_size):\n",
    "#读取datashapley和loo结果\n",
    "    if train_size==100:\n",
    "        tmc = []\n",
    "        f=open(\"exp_fast_datashapley/vals_tmc_mnist_{}.txt\".format(train_size),\"r\")\n",
    "        for line in f:\n",
    "            tmc.append(float(line.strip('\\n')))\n",
    "        # print(len(tmc),tmc)\n",
    "        loo = []\n",
    "        f=open(\"exp_fast_datashapley/vals_loo_mnist_{}.txt\".format(train_size),\"r\")\n",
    "        for line in f:\n",
    "            loo.append(float(line.strip('\\n')))\n",
    "    # print(len(loo),loo)\n",
    "    # print(len(values[i,:,test_x_label[i]]),values[i,:,test_x_label[i]])\n",
    "        perfs =performance_plots(train_data,train_data_label,[values[i,:,test_x_label[i]],tmc,loo], nn.Softmax(dim=-1), test_x[i], test_x_label[i], name =\"exp_fast_datashapley/AFDS_mnist__{}_shuffle_compare\".format(train_size), num_plot_markers=20)\n",
    "    else:\n",
    "        perfs =performance_plots(train_data,train_data_label,[values[i,:,test_x_label[i]]], nn.Softmax(dim=-1), test_x[i], test_x_label[i], name =\"exp_fast_datashapley/AFDS_mnist__{}_shuffle_compare\".format(train_size), num_plot_markers=20)\n",
    "    H_eta.append(perfs)\n",
    "H_std = np.std(np.array(H_eta),axis=0)\n",
    "H_mean = np.mean(np.array(H_eta),axis=0)\n",
    "print('ours:',H_mean[0])\n",
    "print('std:',H_std[0])\n",
    "if train_size==100:\n",
    "    print('DataShapley:',H_mean[1])\n",
    "    print('std:',H_std[1])\n",
    "    print('loo:',H_mean[2])\n",
    "    print('std:',H_std[2])\n",
    "print('random:',H_mean[-1])    \n",
    "print('std:',H_std[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastshap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
